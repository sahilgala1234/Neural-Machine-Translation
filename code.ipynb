{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2gUvvZ8JVQi"
   },
   "source": [
    "# Project - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryhQPCrMJglz"
   },
   "source": [
    "1. Build a Neural Machine Translation model.\n",
    "2. Evaluate your model using BLEU score. <br>\n",
    "Dataset: http://www.manythings.org/anki/fra-eng.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyCBW9aLI6lz"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqobqNwQI4ql",
    "outputId": "58ee6742-c825-4dd2-a29f-9a868df08c1a"
   },
   "outputs": [],
   "source": [
    "# !rm *.zip* -rf\n",
    "# !wget  http://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOqT0FoIJe2r",
    "outputId": "c9269414-ed91-425e-d15e-0b62392dd43c"
   },
   "outputs": [],
   "source": [
    "# !unzip -o fra-eng.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPYz5EBxK7_j"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "C-1ZTl7aL7pf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g98vW-JrJPrR",
    "outputId": "40908459-3127-422c-d840-e410c0e5a2db"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L9yz_VBqMjHf"
   },
   "outputs": [],
   "source": [
    "# split a loaded document into sentences, then shuffle\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t')[:2] for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "A4caBeS-O896"
   },
   "outputs": [],
   "source": [
    "# max number of pairs to consider\n",
    "n_sentence =50000\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    i_sentence=0\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        if i_sentence>=n_sentence:\n",
    "            break\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            \n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            \n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            \n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            \n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        i_sentence+=1\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)\n",
    "cleaned_doc_sentences=clean_pairs(to_pairs(load_doc('fra.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gN0Ux383XwrA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "['go' 'bouge']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_doc_sentences.shape)\n",
    "print(cleaned_doc_sentences[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMULvpiaIDGs"
   },
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2YhijSF8IVRP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2) (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(cleaned_doc_sentences)\n",
    "\n",
    "# split into train/test\n",
    "n_train=int(0.8*n_sentence)\n",
    "train, test = cleaned_doc_sentences[:n_train], cleaned_doc_sentences[n_train:]\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IoCE0xUNdil"
   },
   "source": [
    "## Tokenizing and Transforming Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVpRJDdnRLVX"
   },
   "source": [
    "### *Tokenize*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GuaVDNOINrWZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drshw\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AflhkbXMNnb8"
   },
   "outputs": [],
   "source": [
    "# fit and create tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    max_l=0\n",
    "    l=''\n",
    "    for line in lines:\n",
    "        if(max_l<len(line.split())):\n",
    "            max_l=len(line.split())\n",
    "            l=line.split()\n",
    "    print(\"max line:\",l)\n",
    "    return max_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CISi5w_MOC1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max line: ['is', 'it', 'a', 'yes', 'or', 'a', 'no']\n",
      "English Vocabulary Size: 5930\n",
      "English Max Length: 7\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(cleaned_doc_sentences[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(cleaned_doc_sentences[:, 0])\n",
    "print('English Vocabulary Size:',eng_vocab_size)\n",
    "print('English Max Length:',eng_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j-ukgmzTOSin"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max line: ['il', 'ny', 'a', 'pas', 'de', 'quoi', 'se', 'faire', 'des', 'nuds', 'au', 'cerveau']\n",
      "French Vocabulary Size: 11847\n",
      "French Max Length: 12\n"
     ]
    }
   ],
   "source": [
    "# prepare french tokenizer\n",
    "fra_tokenizer = create_tokenizer(cleaned_doc_sentences[:, 1])\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "fra_length = max_length(cleaned_doc_sentences[:, 1])\n",
    "print('French Vocabulary Size:',fra_vocab_size)\n",
    "print('French Max Length:',fra_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99rk1HUcLFxv"
   },
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tNd12EpeSUSG"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical, pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gDdVdASPOXfV"
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = np.array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aWJl5EGkTAP5"
   },
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(fra_tokenizer, fra_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(fra_tokenizer, fra_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R25mCEFVU4gR"
   },
   "source": [
    "## Define and Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "XeFvyV9eV517"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qk58S9VcVEwj"
   },
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "L4nkX-QvV0M3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 12, 256)           3032832   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               525312    \n",
      "                                                                 \n",
      " repeat_vector (RepeatVecto  (None, 7, 256)            0         \n",
      " r)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 7, 256)            525312    \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 7, 5930)           1524010   \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5607466 (21.39 MB)\n",
      "Trainable params: 5607466 (21.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(fra_vocab_size, eng_vocab_size, fra_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BODIjPO-XDDF"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=15\n",
    "BATCH_SIZE=16\n",
    "MIN_DELTA=0.01\n",
    "PATIENCE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JoTfyTXHXHMO"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',patience=PATIENCE, min_delta=MIN_DELTA)\n",
    "checkpoint = ModelCheckpoint('chkpoint', monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lbAdc_0xXENS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 2.9982\n",
      "Epoch 1: val_loss improved from inf to 2.52995, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 181s 71ms/step - loss: 2.9982 - val_loss: 2.5300\n",
      "Epoch 2/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 2.2168\n",
      "Epoch 2: val_loss improved from 2.52995 to 2.07580, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 176s 70ms/step - loss: 2.2168 - val_loss: 2.0758\n",
      "Epoch 3/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 1.7657\n",
      "Epoch 3: val_loss improved from 2.07580 to 1.81356, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 185s 74ms/step - loss: 1.7657 - val_loss: 1.8136\n",
      "Epoch 4/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 1.4247\n",
      "Epoch 4: val_loss improved from 1.81356 to 1.62766, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 179s 72ms/step - loss: 1.4247 - val_loss: 1.6277\n",
      "Epoch 5/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 1.1438\n",
      "Epoch 5: val_loss improved from 1.62766 to 1.50120, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 189s 76ms/step - loss: 1.1438 - val_loss: 1.5012\n",
      "Epoch 6/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.9188\n",
      "Epoch 6: val_loss improved from 1.50120 to 1.41297, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 193s 77ms/step - loss: 0.9188 - val_loss: 1.4130\n",
      "Epoch 7/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.7458\n",
      "Epoch 7: val_loss improved from 1.41297 to 1.35751, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 187s 75ms/step - loss: 0.7458 - val_loss: 1.3575\n",
      "Epoch 8/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.6119\n",
      "Epoch 8: val_loss improved from 1.35751 to 1.33338, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 197s 79ms/step - loss: 0.6119 - val_loss: 1.3334\n",
      "Epoch 9/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.5095\n",
      "Epoch 9: val_loss improved from 1.33338 to 1.32046, saving model to chkpoint\n",
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chkpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 190s 76ms/step - loss: 0.5095 - val_loss: 1.3205\n",
      "Epoch 10/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.4313\n",
      "Epoch 10: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 188s 75ms/step - loss: 0.4313 - val_loss: 1.3208\n",
      "Epoch 11/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.3691\n",
      "Epoch 11: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 187s 75ms/step - loss: 0.3691 - val_loss: 1.3248\n",
      "Epoch 12/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.3225\n",
      "Epoch 12: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 188s 75ms/step - loss: 0.3225 - val_loss: 1.3345\n",
      "Epoch 13/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.2839\n",
      "Epoch 13: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 188s 75ms/step - loss: 0.2839 - val_loss: 1.3508\n",
      "Epoch 14/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.2563\n",
      "Epoch 14: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 188s 75ms/step - loss: 0.2563 - val_loss: 1.3659\n",
      "Epoch 15/15\n",
      "2500/2500 [==============================] - ETA: 0s - loss: 0.2328\n",
      "Epoch 15: val_loss did not improve from 1.32046\n",
      "2500/2500 [==============================] - 188s 75ms/step - loss: 0.2328 - val_loss: 1.3744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x205c414e670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY,\n",
    "          epochs=EPOCHS, \n",
    "          batch_size=BATCH_SIZE, \n",
    "          validation_data=(testX, testY), \n",
    "#           callbacks=[es]\n",
    "          callbacks=[checkpoint]          \n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "    # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 2 or i>len(raw_dataset)-3:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score with different n-grams\n",
    "    print('BLEU: %f' % corpus_bleu(actual, predicted))\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[me comprendstu], target=[do you understand me], predicted=[do you understand me]\n",
      "src=[tu es toujours vivante], target=[youre still alive], predicted=[youre still alive]\n",
      "src=[restez baisse], target=[stay down], predicted=[stay down]\n",
      "src=[ne moubliez pas], target=[dont forget me], predicted=[dont say me]\n",
      "BLEU: 0.547125\n",
      "BLEU-1: 0.829402\n",
      "BLEU-2: 0.759050\n",
      "BLEU-3: 0.706280\n",
      "BLEU-4: 0.547125\n",
      "\n",
      "test\n",
      "src=[ca depend de toi], target=[its up to you], predicted=[it depends on you]\n",
      "src=[je regrette de tavoir embrasse], target=[i regret kissing you], predicted=[i regret kissing you you]\n",
      "src=[ca arrive], target=[its getting there], predicted=[it happens happen]\n",
      "src=[mon sac fut derobe], target=[my bag was stolen], predicted=[my wife was taken]\n",
      "BLEU: 0.287312\n",
      "BLEU-1: 0.608362\n",
      "BLEU-2: 0.490646\n",
      "BLEU-3: 0.430707\n",
      "BLEU-4: 0.287312\n",
      "\n",
      "time taken to evaluate: 2156.7580366134644\n"
     ]
    }
   ],
   "source": [
    "model = load_model('chkpoint')\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)\n",
    "\n",
    "end = time.time()\n",
    "print(\"time taken to evaluate:\",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
